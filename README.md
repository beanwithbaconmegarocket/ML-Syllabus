# Functions and Optima

Machine Learning creates algorithms that can discover the shape of large datasets, datasets that are much larger than a human could ever compute in their lifetimes. Before we can teach computers to discover his dataset shape, we need to study the shape of smaller datasets. Learn about the most basic function, `y = f(x)` and how it informs our understanding of the shape of any dataset. We will study how to optimize `y = f(x)`, that is, identify where it is largest or smallest, and from a given sample we can compute the gradient that leads us to the optimum.

- Functions
- Intro to Optimization
- Gradient Descent


# Linear Algebra and High Dimensionality

All Machine Learning depends on the relationships of data in a high dimensional space. Linear algegbra is primarily the tool used for transforming, visualizing, and understanding high dimensional spaces. 

- Linear Algebra
- Matrices
- Transforms


# Linear Regression

The most basic skill in the Machine Learning expert toolset, regression is the simple mathematical prediction of unknown values using previously known information.


# Data

Data is almost always a spreadsheet. We need methods to get data into spreadsheets, annotate it, and convert it into forms that are efficiently computable by Machine Learning programming languages. We need to identify and trim outliers that break our algorithms, understand the shape and structure of the data, and understand the nature of complexity in the context of large datasets.

- Information as Vector
- Data as Matrix
- Data as Function
- Gathering Data
- Transmitting and Transforming Data
- Computability of Large Datasets
- What is Machine Learning


# Supervised Learning

Classical Machine Learning is performed over datasets using a training set to configure the algorithms and a testing set to measure the model effectiveness. When the results of the test set are maximized, we assume that the model will be good on real data. This is not always the case. Careful configuration of the training and testing sets improve results. In supervised learning, we always know the right answer for the entire training and testing data set, and the goal is for our model to tell us the right answer for data that is outside of our original starting information.


- Training and Testing sets
- Accuracy vs Precision and ROC
- Type of Supervised Learning


# Clustering

Clustering, in contrast to supervised learning, never contains correct result annotations. Instead, clustering algorithms provide greater insight into a dataset, which the ML expert must create explanations for by examining the data.

- Types of Unsupervised Learning
- Clustering Algorithms
- Discovering Hidden Patterns (Data Mining)


# Dimensionality Reduction

Every ML dataset is too big to grasp. Every ML algorithm is designed to reduce the complexity of the dataset, either intuitively or using some method that isn't immediately obvious to the builder. Even when we have a model that describes the data well, visualizing it remains extremely difficult. Some intentional methods of dimensionality reduction made data easier to understand, model, and share with others.

- Eigenvectors and Eigenvalues
- Singular Value Decomposition
- Making high dimensional data visualizable


# Reinforcement Learning

The most traditional form of "machine learning", reinforcement learning attempts to improve the result of an evaluation function on an input. For each input, there is a best output. By reinforcing the best output over a large number of iterations, the model can be trained.

- Self Driving Cars (SDC): Course correction
- SDC: Acceleration/Deceleration


# Support Vector Machines

Support vector machines enable identifying inputs from two different classes with a very high degree of accuracy. They can separate the two classes in complicated and unintuitive ways - the most powerful form of regression currently available.

- High dimensional regression
- Kernel Trick: Nonlinear linear regression
- Binary classification


# Evolutionary Algorithms

Evolutionary algorithms calculate results using machine learning models that have been inspired by the study of existing phenomena in nature.

- Simulated Annealing
- Ant Colony Optimization
- Genetic Algorithms


# Neural Networks

The backbone of the modern excitement in Machine Learning, recent advancements in high performance computing have made neural nets interesting again. The basic design is based on neuroscience of the 1960s, but neural nets weren't capable of much except for handwriting recognition for a long time.

- Perceptron
- Historical Use Cases
- Backpropagation


# Deep CNNs

Deep convolutional neural networks are how Google can now identify you, your family members, your dog, and your favorite restaurants from photos you take through the day. By the time you are finished with Lambda School Machine Learning Academy you will be able to reproduce Hinton's important research that got everyone interested in neural networks again.

- CNN
- Layer Selection
- TensorFlow
- Hinton's Benchmark


# Computer Vision

Photographs are just points in an extremely high-dimensional space. Learn how to process and perform calculations on images.

- Image Processing
- Eigenfaces
- Object Recognition


# Computer Vision for Self Driving Cars

Automatically piloted cars are exciting upcoming news. They are enabled by careful advances in range finding, gps assisted mapping, computer vision, and input control for vehicles.

- Cost Function
- Edge Detection
- Object Recognition


# Simultaneous Location and Mapping (SLAM)

When GPS isn't available, computer visual systems need to create and maintain a map of their own environment. Learn how to build and maintain that representation for robotic navigation.

- Visual Reconstruction
- Mapping


# Data Visualization

It is difficult to write a good machine learning model when you can't visualize the data that it represents. More importantly, presenting and pitching what you've learned about your data, from your ML model, keeps you in a job. Drawing cool pictures of incomprehensible datasets is one of the best parts of our job.

- Datasets
- Dimensionality Reduction
- Topologies
- Graphing in R
- Graphing in Matlab


# Natural Language Processing

Natural Language Processing (NLP) helps Alexa understand what you are asking for. It is also huge in automatic document tagging and identification, a feature that is useful for libraries, legal, and law enforcement.

"Classical texts include:


https://nlp.stanford.edu/fsnlp/?lipi=urn%3Ali%3Apage%3Ad_flagship3_messaging%3B4SlkMkv%2BQ3%2BgXH%2FNBoxe%2Bw%3D%3D


and


https://web.stanford.edu/~jurafsky/slp3/?lipi=urn%3Ali%3Apage%3Ad_flagship3_messaging%3B4SlkMkv%2BQ3%2BgXH%2FNBoxe%2Bw%3D%3D"
- Building Corpuses
- Tokenization
- Tagging
- Named Entity Recognition
- Semantic Processing
- Word Similarity
- Keyword Extraction
- Document Classification


# Building Alexa

How to build Alexa?


# Map/reduce



- Large distributed datasets
- Parallelized Map
- Recursive reduce
- CouchDB


# Random Forests




# Ensemble Stacking




# Sparc/Hadoop




# TensorFlow: CNN




# TensorFlow: Other




# Google TPUs




# Amazon AI Services




# Project Kaggle Contests




# Project Predicting Real Estate Market




# Project NLP Email Content Clustering




# Project Copy Deep Dream Generator




# Project NN Recognizing Written Digits




# Project CNN Reproduce Hinton's Seminal Image Machine





# Precourse





